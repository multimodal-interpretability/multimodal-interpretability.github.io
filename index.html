<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="files/jumbotron.css" rel="stylesheet">
    <script src="js/main.js"></script>
    <script src="js/scroll.js"></script>

    <style>
        @media (max-width: 767px) {
            .col-md-2 .img-fluid {
                margin-bottom: 20px;
            }
        }

        .project-link {
            font-weight: bold;
            color: #f2a23f;
            margin-right: 10px;
        }

        .project-link:hover {
            color: #f7af57;
        }
        
        .news-link {
            font-weight: bold;
            color: #6b91cf;
        }

        .news-link:hover {
            color: ##87b3fa;
        }
        
    </style>
</head>

<title>Multimodal Interpretability</title>

<body>
    <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
        <a class="navbar-brand" href="#">Multimodal Interpretability</a>
        <div class="collapse navbar-collapse" id="navbarToggle">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="#Publications">Publications</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#News">News</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Contact">Contact</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://accessibility.mit.edu" target="_blank">Accessibility</a>
                </li>
            </ul>
        </div>
    </nav>

    <div class="container" style="padding-top: 100px;">
        <p>We are a team of researchers based out of MIT CSAIL with close collaborators at other institutions (in both academia and industry). We build tools for reverse-engineering AI systems to make them more transparent and steerable. We are particularly focused on <em>Automated Interpretability</em> approaches that use neural models themselves to assist with model understanding tasks.</p>
    </div>

    <div class="container" id="Publications" style="padding-top: 120px; margin-top: -80px;">
        <h3>Recent Publications</h3>
        <div class="row" style="padding-top: 20px;">
            <div class="col-md-2">
                <img src="static/images/MAIA_schematic.png" alt="MAIA" class="img-fluid">
            </div>
            <div class="col-md-10">
                <h4>A Multimodal Automated Interpretability Agent</h4>
                <p>Tamar Rott Shaham*, Sarah Schwettmann*, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, Antonio Torralba. ICML 2024.</p>
                <p>An agent that autonomously conducts experiments on other systems to explain their behavior, by composing interpretability subroutines into Python programs.</p>
                <p> <a href="https://arxiv.org/pdf/2404.14394.pdf" target="_blank" class="project-link">Paper</a>
    <a href="https://multimodal-interpretability.csail.mit.edu/maia/" target="_blank" class="project-link">Project Page</a>
 <a href="https://multimodal-interpretability.csail.mit.edu/maia/experiment-browser/" target="_blank" class="project-link">Experiment browser</a> 
</p>
            </div>
        </div>
        <div class="row" style="padding-top: 20px;">
            <div class="col-md-2">
                <img src="static/images/FIND_gif.gif" alt="FIND" class="img-fluid">
            </div>
            <div class="col-md-10">
                <h4>FIND: A Function Description Benchmark for Evaluating Interpretability Methods</h4>
                <p>Sarah Schwettmann*, Tamar Rott Shaham*, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba. NeurIPS 2023.</p>
                <p>An interactive dataset of functions resembling subcomputations inside trained neural networks, for validating and comparing open-ended labeling tools, and a new method that uses Automated Interpretability Agents to explain other systems.</p>
                <p><a href="https://arxiv.org/abs/2309.03886" target="_blank" class="project-link">Paper</a> <a href="https://github.com/multimodal-interpretability/FIND" target="_blank" class="project-link">Dataset</a> <a href="https://multimodal-interpretability.csail.mit.edu/FIND-benchmark/" target="_blank" class="project-link">Project Page</a> <a href="https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103" target="_blank" class="project-link">News</a></p>
            </div>
        </div>
        <div class="row" style="padding-top: 20px;">
            <div class="col-md-2">
                <img src="static/images/car_circuit_inception.png" alt="Visual Circuits" class="img-fluid">
            </div>
            <div class="col-md-10">
                <h4>Automatic Discovery of Visual Circuits</h4>
                <p>Achyuta Rajaram*, Neil Chowdhury*, Antonio Torralba, Jacob Andreas, Sarah Schwettmann. NeurIPS ATTRIB 2023. </p>
                <p>We introduce a new technique for automatically discovering subgraphs of vision models that detect concepts.</p>
                <p><a href="https://arxiv.org/pdf/2404.14349.pdf" target="_blank" class="project-link">Paper</a> <a href="https://innotechtoday.com/gen-zs-search-for-ethical-ai-at-the-regeneron-science-talent-search/" target="_blank" class="project-link">News</a></p>
            </div>
        </div>
        <div class="row" style="padding-top: 20px;">
            <div class="col-md-2">
                <img src="static/images/mmns_teaser.png" alt="Multimodal neurons" class="img-fluid">
            </div>
            <div class="col-md-10">
                <h4>Multimodal Neurons in Pretrained Text-Only Transformers</h4>
                <p>Sarah Schwettmann*, Neil Chowdhury*, Samuel Klein, David Bau, Antonio Torralba. ICCV CVCL 2023 (Oral).</p>
                <p>We find multimodal neurons in a transformer pretrained only on language. When image representations are aligned to the language model, these neurons activate on specific image features and inject related text into the model's next token prediction.</p>
                <p><a href="https://arxiv.org/abs/2308.01544" target="_blank" class="project-link">Paper</a> <a href="https://multimodal-interpretability.csail.mit.edu/Multimodal-Neurons-in-Text-Only-Transformers/" target="_blank" class="project-link">Project Page</a></p>
            </div>
        </div>
        <div class="row" style="padding-top: 20px;">
            <div class="col-md-2">
                <img src="static/images/MILAN.png" alt="MILAN" class="img-fluid">
            </div>
            <div class="col-md-10">
                <h4>Natural Language Descriptions of Deep Visual Features</h4>
                <p>Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, Jacob Andreas. ICLR 2022 (Oral).</p>
                <p>We introduce a procedure that automatically labels neurons in deep networks with open-ended, compositional, natural language descriptions of their function.</p>
                <p><a href="https://arxiv.org/abs/2201.11114" target="_blank" class="project-link">Paper</a> <a href="https://milan.csail.mit.edu/" target="_blank" class="project-link">Project Page</a> <a href="https://news.mit.edu/2022/explainable-machine-learning-0127" target="_blank" class="project-link">News</a></p>
            </div>
        </div>
    </div>

    <div class="container" id="News" style="padding-top: 120px; margin-top: -80px;">
        <h3>Recent News</h3>
        <ul>
            <li>November 2024: NNN <a href="https://github.com/multimodal-interpretability/nnn" target="_blank" class="news-link">python package</a> released, and Sarah, Neil and Franklin travel to Miami to present NNN at EMNLP.</li>
            <li>October 2024: Sarah and Neil launch <a href="https://transluce.org/" target="_blank" class="news-link">Transluce</a>, a new nonprofit research lab developing open and scalable tools for understanding AI systems.</li> 
            <li>September 2024: <a href="https://arxiv.org/abs/2410.24114" target="_blank" class="news-link">Nearest Neighbor Normalization</a> is accepted to the EMNLP 2024 Main Conference!</li>  
            <li>July 2024: <a href="https://multimodal-interpretability.csail.mit.edu/maia/" target="_blank" class="news-link">MAIA</a> is featured in <a href="https://news.mit.edu/2024/mit-researchers-advance-automated-interpretability-ai-models-maia-0723" target="_blank" class="news-link">MIT News</a> and presented at ICML 2024. 
            <li>June 2024: Jake joins as a MIT CSAIL AI Interpretability Intern!</li>
            <li>May 2024:  <a href="https://arxiv.org/pdf/2404.14394" target="_blank" class="news-link">MAIA</a> is accepted to ICML 2024!</li>
            <li>April 2024: Sarah and Neil are co-organizing the Visual Emergent Abilities and Visual Limits of Foundation Models Workshop at <a href="https://sites.google.com/view/eval-fomo-24/home" target="_blank" class="news-link">ECCV 2024</a>. 
            <li>March 2024: Achyuta wins first place nationally and a $250,000 prize at <a href="https://www.societyforscience.org/regeneron-sts/2024-sts-winners/" target="_blank" class="news-link">Regeneron STS</a> for his project on Automatic Discovery of Visual Circuits, done as part of our MIT CSAIL AI Interpretability Internship Program.</li>
            <li>January 2024: Neil joins the <a href="https://openai.com/safety/preparedness" target="_blank" class="news-link">Preparedness Team</a> at OpenAI.</li>
            <li>January 2024: Automated Interpretability Agents and FIND are featured in <a href="https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103" target="_blank" class="news-link">MIT News</a>.</li>
            <li>December 2023: Sarah and Tamar present <a href="https://arxiv.org/abs/2309.03886" target="_blank" class="news-link">FIND</a> at NeurIPS in New Orleans.</li>
        </ul>
    </div>

    <div class="container" id="Contact" style="padding-top: 120px; margin-top: -80px; padding-bottom: 120px;">
        <h3>Contact</h3>
        <a href="mailto:multimodalinterpretability@mit.edu" class="news-link">multimodalinterpretability@mit.edu</a>
    </div>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script>showPubs(1);</script>
    <script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000, offset: 100});</script>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>
</html>
